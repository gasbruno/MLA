{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86240f82",
   "metadata": {},
   "source": [
    "# Practical work 3 : MLP, part 2\n",
    "\n",
    "## 2-layers MLP learning \n",
    "\n",
    "Le modèle construit dans le [premier notebook](./Notebook_C4_1.ipynb) n'est pas adapté pour réaliser des boucles d'apprentissage.\n",
    "Dans ce Notebook, on se propose de l'encapsuler dans une classe dotée des méthodes `forward` et `backward`.\n",
    "\n",
    "Ecrire la classe `XORModel` permettant de réaliser un apprentissage comme suit:\n",
    "\n",
    "```python\n",
    "    xor_model = MLPModel()\n",
    "    L = []\n",
    "    lr = 0.01\n",
    "    num_epochs = 1000\n",
    "    for epoch in range(num_epochs = 1000):\n",
    "        loss = xor_model.forward()\n",
    "        L.append(loss)\n",
    "        xor_model.backward()\n",
    "        xor_model.optimizerSGD(lr)\n",
    "        print(f\"Epoch {epoch} - Loss: {loss}\")\n",
    "```\n",
    "\n",
    "L'optimizer est de type SGD (Stochastic Gradient Descent): Les points sont modifiés après chaque passage d'un exemple.\n",
    "\n",
    "Schéma du réseau à réaliser est le même que dans le précédent Notebook:\n",
    "\n",
    "<img src=\"./MLP2-a.jpg\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69c71343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import op\n",
    "\n",
    "class MLPModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Inputs\n",
    "        self.x1 = op.PlaceHolder(1.0)\n",
    "        self.x2 = op.PlaceHolder(-1.0)\n",
    "\n",
    "        # Desired outputs\n",
    "        self.y2_1 = op.PlaceHolder(0)\n",
    "        self.y2_2 = op.PlaceHolder(1.0)\n",
    "\n",
    "        # Layer 1 - Neuron 1\n",
    "        self.w1_11 = op.Parameter(-1)\n",
    "        self.w1_12 = op.Parameter(2)\n",
    "        self.mul1_11 = op.Mul()\n",
    "        self.mul1_12 = op.Mul()\n",
    "        self.add1_11 = op.Add()\n",
    "        self.b1_1 = op.Parameter(4)\n",
    "        self.add1_12 = op.Add()\n",
    "        self.sigma1_1 = op.Tanh()\n",
    "\n",
    "        # Layer 1 - Neuron 2\n",
    "        self.w1_21 = op.Parameter(-1)\n",
    "        self.w1_22 = op.Parameter(1)\n",
    "        self.mul1_21 = op.Mul()\n",
    "        self.mul1_22 = op.Mul()\n",
    "        self.add1_21 = op.Add()\n",
    "        self.b1_2 = op.Parameter(-1)\n",
    "        self.add1_22 = op.Add()\n",
    "        self.sigma1_2 = op.Tanh()\n",
    "\n",
    "        # Layer 2 - Neuron 1\n",
    "        self.w2_11 = op.Parameter(-1)\n",
    "        self.w2_12 = op.Parameter(2)\n",
    "        self.mul2_11 = op.Mul()\n",
    "        self.mul2_12 = op.Mul()\n",
    "        self.add2_11 = op.Add()\n",
    "        self.b2_1 = op.Parameter(4)\n",
    "        self.add2_12 = op.Add()\n",
    "        self.sigma2_1 = op.Tanh()\n",
    "\n",
    "        # Layer 2 - Neuron 2\n",
    "        self.w2_21 = op.Parameter(-1)\n",
    "        self.w2_22 = op.Parameter(1)\n",
    "        self.mul2_21 = op.Mul()\n",
    "        self.mul2_22 = op.Mul()\n",
    "        self.add2_21 = op.Add()\n",
    "        self.b2_2 = op.Parameter(-1)\n",
    "        self.add2_22 = op.Add()\n",
    "        self.sigma2_2 = op.Tanh()\n",
    "\n",
    "        # Error / Loss\n",
    "        self.err1 = op.Sub()\n",
    "        self.err2 = op.Sub()\n",
    "        self.mse1 = op.Square()\n",
    "        self.mse2 = op.Square()\n",
    "        self.mseT = op.Add()\n",
    "\n",
    "    def forward(self):\n",
    "        # Forward Layer 1 neuron 1\n",
    "        e1_11 = self.mul1_11(self.x1(), self.w1_11())\n",
    "        e1_12 = self.mul1_12(self.x2(), self.w1_12())\n",
    "        v1_10 = self.add1_11(e1_11, e1_12)\n",
    "        v1_1 = self.add1_12(v1_10, self.b1_1())\n",
    "        y1_1 = self.sigma1_1(v1_1)\n",
    "\n",
    "        # Forward Layer 1 neuron 2\n",
    "        e1_21 = self.mul1_21(self.x1(), self.w1_21())\n",
    "        e1_22 = self.mul1_22(self.x2(), self.w1_22())\n",
    "        v1_20 = self.add1_21(e1_21, e1_22)\n",
    "        v1_2 = self.add1_22(v1_20, self.b1_2())\n",
    "        y1_2 = self.sigma1_2(v1_2)\n",
    "\n",
    "        # Forward Layer 2 neuron 1\n",
    "        e2_11 = self.mul2_11(y1_1, self.w2_11())\n",
    "        e2_12 = self.mul2_12(y1_2, self.w2_12())\n",
    "        v2_10 = self.add2_11(e2_11, e2_12)\n",
    "        v2_1 = self.add2_12(v2_10, self.b2_1())\n",
    "        ypred_1 = self.sigma2_1(v2_1)\n",
    "\n",
    "        # Forward Layer 2 neuron 2\n",
    "        e2_21 = self.mul2_21(y1_1, self.w2_21())\n",
    "        e2_22 = self.mul2_22(y1_2, self.w2_22())\n",
    "        v2_20 = self.add2_21(e2_21, e2_22)\n",
    "        v2_2 = self.add2_22(v2_20, self.b2_2())\n",
    "        ypred_2 = self.sigma2_2(v2_2)\n",
    "\n",
    "        # Error\n",
    "        error1 = self.err1(ypred_1, self.y2_1())\n",
    "        error2 = self.err2(ypred_2, self.y2_2())\n",
    "        mserror1 = self.mse1(error1)\n",
    "        mserror2 = self.mse2(error2)\n",
    "        return self.mseT(mserror1, mserror2)\n",
    "\n",
    "    def backward(self):\n",
    "        g_L = 1\n",
    "\n",
    "        # Backward Layer 2 neuron 1\n",
    "        g_mserror1 = self.mseT.grad1(g_L)\n",
    "        g_error1 = self.mse1.grad(g_mserror1)\n",
    "        g_ypred_1 = self.err1.grad1(g_error1)\n",
    "        g_v2_1 = self.sigma2_1.grad(g_ypred_1)\n",
    "        g_v2_10 = self.add2_12.grad1(g_v2_1)\n",
    "        self.g_b2_1 = self.add2_12.grad2(g_v2_1)\n",
    "        g_e2_11 = self.mul2_11.grad1(g_v2_10)\n",
    "        self.g_w2_11 = self.mul2_11.grad2(g_v2_10)\n",
    "        g_e2_12 = self.mul2_12.grad1(g_v2_10)\n",
    "        self.g_w2_12 = self.mul2_12.grad2(g_v2_10)\n",
    "\n",
    "        # Backward Layer 2 neuron 2\n",
    "        g_mserror2 = self.mseT.grad2(g_L)\n",
    "        g_error2 = self.mse2.grad(g_mserror2)\n",
    "        g_ypred_2 = self.err2.grad1(g_error2)\n",
    "        g_v2_2 = self.sigma2_2.grad(g_ypred_2)\n",
    "        g_v2_20 = self.add2_22.grad1(g_v2_2)\n",
    "        self.g_b2_2 = self.add2_22.grad2(g_v2_2)\n",
    "        g_e2_21 = self.mul2_21.grad1(g_v2_20)\n",
    "        self.g_w2_21 = self.mul2_21.grad2(g_v2_20)\n",
    "        g_e2_22 = self.mul2_22.grad1(g_v2_20)\n",
    "        self.g_w2_22 = self.mul2_22.grad2(g_v2_20)\n",
    "\n",
    "        # Backward Layer 1 neuron 1\n",
    "        g_v1_1 = self.sigma1_1.grad(g_e2_11 + g_e2_21)\n",
    "        g_v1_10 = self.add1_12.grad1(g_v1_1)\n",
    "        self.g_b1_1 = self.add1_12.grad2(g_v1_1)\n",
    "        self.g_w1_11 = self.mul1_11.grad2(g_v1_10)\n",
    "        self.g_w1_12 = self.mul1_12.grad2(g_v1_10)\n",
    "\n",
    "        # Backward Layer 1 neuron 2\n",
    "        g_v1_2 = self.sigma1_2.grad(g_e2_12 + g_e2_22)\n",
    "        g_v1_20 = self.add1_22.grad1(g_v1_2)\n",
    "        self.g_b1_2 = self.add1_22.grad2(g_v1_2)\n",
    "        self.g_w1_21 = self.mul1_21.grad2(g_v1_20)\n",
    "        self.g_w1_22 = self.mul1_22.grad2(g_v1_20)\n",
    "\n",
    "    def optimizerSGD(self, lr):\n",
    "        \"\"\"Stochastic Gradient Descent\"\"\"\n",
    "        self.w2_11.update(self.w2_11() - lr * self.g_w2_11)\n",
    "        self.w2_12.update(self.w2_12() - lr * self.g_w2_12)\n",
    "        self.b2_1.update(self.b2_1() - lr * self.g_b2_1)\n",
    "        self.w2_21.update(self.w2_21() - lr * self.g_w2_21)\n",
    "        self.w2_22.update(self.w2_22() - lr * self.g_w2_22)\n",
    "        self.b2_2.update(self.b2_2() - lr * self.g_b2_2)\n",
    "        self.w1_11.update(self.w1_11() - lr * self.g_w1_11)\n",
    "        self.w1_12.update(self.w1_12() - lr * self.g_w1_12)\n",
    "        self.b1_1.update(self.b1_1() - lr * self.g_b1_1)\n",
    "        self.w1_21.update(self.w1_21() - lr * self.g_w1_21)\n",
    "        self.w1_22.update(self.w1_22() - lr * self.g_w1_22)\n",
    "        self.b1_2.update(self.b1_2() - lr * self.g_b1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb3837a",
   "metadata": {},
   "source": [
    "## Go !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c648e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator -> PlaceHolder\n",
      "Operator -> PlaceHolder\n",
      "Operator -> PlaceHolder\n",
      "Operator -> PlaceHolder\n",
      "Operator -> Mul\n",
      "Operator -> Mul\n",
      "Operator -> Add\n",
      "Operator -> Add\n",
      "Operator -> Tanh\n",
      "Operator -> Mul\n",
      "Operator -> Mul\n",
      "Operator -> Add\n",
      "Operator -> Add\n",
      "Operator -> Tanh\n",
      "Operator -> Mul\n",
      "Operator -> Mul\n",
      "Operator -> Add\n",
      "Operator -> Add\n",
      "Operator -> Tanh\n",
      "Operator -> Mul\n",
      "Operator -> Mul\n",
      "Operator -> Add\n",
      "Operator -> Add\n",
      "Operator -> Tanh\n",
      "Operator -> Sub\n",
      "Operator -> Sub\n",
      "Operator -> Square\n",
      "Operator -> Square\n",
      "Operator -> Add\n",
      "Epoch 0 - Loss: 4.686705513091586\n",
      "Epoch 1 - Loss: 4.679783282987222\n",
      "Epoch 2 - Loss: 4.672581653881472\n",
      "Epoch 3 - Loss: 4.665087683152811\n",
      "Epoch 4 - Loss: 4.6572880157943795\n",
      "Epoch 5 - Loss: 4.649168924656944\n",
      "Epoch 6 - Loss: 4.640716364614289\n",
      "Epoch 7 - Loss: 4.631916043202434\n",
      "Epoch 8 - Loss: 4.622753510592269\n",
      "Epoch 9 - Loss: 4.613214272059152\n",
      "Epoch 10 - Loss: 4.603283926394587\n",
      "Epoch 11 - Loss: 4.592948333938928\n",
      "Epoch 12 - Loss: 4.582193818066329\n",
      "Epoch 13 - Loss: 4.571007403979579\n",
      "Epoch 14 - Loss: 4.559377098517425\n",
      "Epoch 15 - Loss: 4.547292214272042\n",
      "Epoch 16 - Loss: 4.5347437405793976\n",
      "Epoch 17 - Loss: 4.521724762789109\n",
      "Epoch 18 - Loss: 4.508230929546336\n",
      "Epoch 19 - Loss: 4.494260965530722\n",
      "Epoch 20 - Loss: 4.479817224114728\n",
      "Epoch 21 - Loss: 4.464906270675296\n",
      "Epoch 22 - Loss: 4.44953948282223\n",
      "Epoch 23 - Loss: 4.433733648679644\n",
      "Epoch 24 - Loss: 4.417511538772514\n",
      "Epoch 25 - Loss: 4.400902421370974\n",
      "Epoch 26 - Loss: 4.383942485838076\n",
      "Epoch 27 - Loss: 4.366675134289167\n",
      "Epoch 28 - Loss: 4.34915109952567\n",
      "Epoch 29 - Loss: 4.331428347657215\n",
      "Epoch 30 - Loss: 4.3135717279502\n",
      "Epoch 31 - Loss: 4.295652340929007\n",
      "Epoch 32 - Loss: 4.277746608926955\n",
      "Epoch 33 - Loss: 4.259935050893124\n",
      "Epoch 34 - Loss: 4.242300784355105\n",
      "Epoch 35 - Loss: 4.224927800304952\n",
      "Epoch 36 - Loss: 4.207899079042399\n",
      "Epoch 37 - Loss: 4.191294633903551\n",
      "Epoch 38 - Loss: 4.17518958256968\n",
      "Epoch 39 - Loss: 4.15965235004999\n",
      "Epoch 40 - Loss: 4.144743102216692\n",
      "Epoch 41 - Loss: 4.130512494013478\n",
      "Epoch 42 - Loss: 4.117000793636325\n",
      "Epoch 43 - Loss: 4.104237415773044\n",
      "Epoch 44 - Loss: 4.092240866799569\n",
      "Epoch 45 - Loss: 4.081019076217727\n",
      "Epoch 46 - Loss: 4.0705700646657865\n",
      "Epoch 47 - Loss: 4.060882881693108\n",
      "Epoch 48 - Loss: 4.051938737150313\n",
      "Epoch 49 - Loss: 4.043712248340699\n",
      "Epoch 50 - Loss: 4.03617272990909\n",
      "Epoch 51 - Loss: 4.029285463123467\n",
      "Epoch 52 - Loss: 4.023012893823734\n",
      "Epoch 53 - Loss: 4.017315722058069\n",
      "Epoch 54 - Loss: 4.0121538598033535\n",
      "Epoch 55 - Loss: 4.007487245099881\n",
      "Epoch 56 - Loss: 4.003276510789634\n",
      "Epoch 57 - Loss: 3.9994835135845266\n",
      "Epoch 58 - Loss: 3.996071734455374\n",
      "Epoch 59 - Loss: 3.993006564568727\n",
      "Epoch 60 - Loss: 3.9902554925581466\n",
      "Epoch 61 - Loss: 3.9877882091831385\n",
      "Epoch 62 - Loss: 3.9855766447713368\n",
      "Epoch 63 - Loss: 3.9835949535794946\n",
      "Epoch 64 - Loss: 3.9818194576087187\n",
      "Epoch 65 - Loss: 3.9802285606695196\n",
      "Epoch 66 - Loss: 3.9788026417557445\n",
      "Epoch 67 - Loss: 3.977523935148304\n",
      "Epoch 68 - Loss: 3.9763764031862356\n",
      "Epoch 69 - Loss: 3.9753456063416253\n",
      "Epoch 70 - Loss: 3.9744185741236193\n",
      "Epoch 71 - Loss: 3.9735836794086588\n",
      "Epoch 72 - Loss: 3.972830518034884\n",
      "Epoch 73 - Loss: 3.9721497948894022\n",
      "Epoch 74 - Loss: 3.9715332172375377\n",
      "Epoch 75 - Loss: 3.970973395673148\n",
      "Epoch 76 - Loss: 3.9704637527897955\n",
      "Epoch 77 - Loss: 3.969998439467219\n",
      "Epoch 78 - Loss: 3.9695722585213895\n",
      "Epoch 79 - Loss: 3.969180595367114\n",
      "Epoch 80 - Loss: 3.968819355279001\n",
      "Epoch 81 - Loss: 3.9684849068013266\n",
      "Epoch 82 - Loss: 3.968174030842741\n",
      "Epoch 83 - Loss: 3.967883874992478\n",
      "Epoch 84 - Loss: 3.9676119126062344\n",
      "Epoch 85 - Loss: 3.9673559062289527\n",
      "Epoch 86 - Loss: 3.967113874945474\n",
      "Epoch 87 - Loss: 3.9668840652767874\n",
      "Epoch 88 - Loss: 3.966664925267554\n",
      "Epoch 89 - Loss: 3.9664550814389354\n",
      "Epoch 90 - Loss: 3.966253318308513\n",
      "Epoch 91 - Loss: 3.9660585602058873\n",
      "Epoch 92 - Loss: 3.9658698551379348\n",
      "Epoch 93 - Loss: 3.9656863604814627\n",
      "Epoch 94 - Loss: 3.9655073303031823\n",
      "Epoch 95 - Loss: 3.9653321041271883\n",
      "Epoch 96 - Loss: 3.965160096988875\n",
      "Epoch 97 - Loss: 3.9649907906311537\n",
      "Epoch 98 - Loss: 3.964823725714262\n",
      "Epoch 99 - Loss: 3.9646584949244286\n",
      "Epoch 100 - Loss: 3.9644947368791428\n",
      "Epoch 101 - Loss: 3.964332130738091\n",
      "Epoch 102 - Loss: 3.964170391438961\n",
      "Epoch 103 - Loss: 3.9640092654862964\n",
      "Epoch 104 - Loss: 3.9638485272297204\n",
      "Epoch 105 - Loss: 3.963687975575037\n",
      "Epoch 106 - Loss: 3.9635274310781194\n",
      "Epoch 107 - Loss: 3.963366733377248\n",
      "Epoch 108 - Loss: 3.9632057389245956\n",
      "Epoch 109 - Loss: 3.963044318982094\n",
      "Epoch 110 - Loss: 3.962882357850918\n",
      "Epoch 111 - Loss: 3.9627197513073456\n",
      "Epoch 112 - Loss: 3.9625564052209494\n",
      "Epoch 113 - Loss: 3.962392234333802\n",
      "Epoch 114 - Loss: 3.962227161181896\n",
      "Epoch 115 - Loss: 3.962061115142144\n",
      "Epoch 116 - Loss: 3.961894031590247\n",
      "Epoch 117 - Loss: 3.961725851156461\n",
      "Epoch 118 - Loss: 3.961556519067766\n",
      "Epoch 119 - Loss: 3.9613859845663244\n",
      "Epoch 120 - Loss: 3.9612142003952435\n",
      "Epoch 121 - Loss: 3.961041122343758\n",
      "Epoch 122 - Loss: 3.9608667088448373\n",
      "Epoch 123 - Loss: 3.9606909206190433\n",
      "Epoch 124 - Loss: 3.960513720359189\n",
      "Epoch 125 - Loss: 3.9603350724510005\n",
      "Epoch 126 - Loss: 3.9601549427255165\n",
      "Epoch 127 - Loss: 3.959973298239477\n",
      "Epoch 128 - Loss: 3.95979010708039\n",
      "Epoch 129 - Loss: 3.9596053381933554\n",
      "Epoch 130 - Loss: 3.9594189612270445\n",
      "Epoch 131 - Loss: 3.95923094639657\n",
      "Epoch 132 - Loss: 3.959041264361225\n",
      "Epoch 133 - Loss: 3.958849886115304\n",
      "Epoch 134 - Loss: 3.9586567828904453\n",
      "Epoch 135 - Loss: 3.958461926068096\n",
      "Epoch 136 - Loss: 3.9582652871008777\n",
      "Epoch 137 - Loss: 3.9580668374417614\n",
      "Epoch 138 - Loss: 3.9578665484801094\n",
      "Epoch 139 - Loss: 3.9576643914837106\n",
      "Epoch 140 - Loss: 3.9574603375460877\n",
      "Epoch 141 - Loss: 3.9572543575383934\n",
      "Epoch 142 - Loss: 3.9570464220653183\n",
      "Epoch 143 - Loss: 3.956836501424493\n",
      "Epoch 144 - Loss: 3.956624565568921\n",
      "Epoch 145 - Loss: 3.9564105840720294\n",
      "Epoch 146 - Loss: 3.9561945260949956\n",
      "Epoch 147 - Loss: 3.9559763603560025\n",
      "Epoch 148 - Loss: 3.9557560551011597\n",
      "Epoch 149 - Loss: 3.9555335780768326\n",
      "Epoch 150 - Loss: 3.9553088965031433\n",
      "Epoch 151 - Loss: 3.955081977048451\n",
      "Epoch 152 - Loss: 3.9548527858046336\n",
      "Epoch 153 - Loss: 3.954621288263014\n",
      "Epoch 154 - Loss: 3.9543874492907594\n",
      "Epoch 155 - Loss: 3.95415123310767\n",
      "Epoch 156 - Loss: 3.9539126032631997\n",
      "Epoch 157 - Loss: 3.9536715226136163\n",
      "Epoch 158 - Loss: 3.953427953299221\n",
      "Epoch 159 - Loss: 3.953181856721506\n",
      "Epoch 160 - Loss: 3.9529331935202094\n",
      "Epoch 161 - Loss: 3.952681923550154\n",
      "Epoch 162 - Loss: 3.952428005857834\n",
      "Epoch 163 - Loss: 3.9521713986576663\n",
      "Epoch 164 - Loss: 3.951912059307851\n",
      "Epoch 165 - Loss: 3.9516499442857893\n",
      "Epoch 166 - Loss: 3.951385009162999\n",
      "Epoch 167 - Loss: 3.95111720857948\n",
      "Epoch 168 - Loss: 3.950846496217478\n",
      "Epoch 169 - Loss: 3.9505728247745986\n",
      "Epoch 170 - Loss: 3.9502961459362296\n",
      "Epoch 171 - Loss: 3.9500164103472133\n",
      "Epoch 172 - Loss: 3.949733567582741\n",
      "Epoch 173 - Loss: 3.9494475661184048\n",
      "Epoch 174 - Loss: 3.94915835329938\n",
      "Epoch 175 - Loss: 3.948865875308672\n",
      "Epoch 176 - Loss: 3.9485700771344057\n",
      "Epoch 177 - Loss: 3.9482709025360805\n",
      "Epoch 178 - Loss: 3.9479682940097787\n",
      "Epoch 179 - Loss: 3.947662192752241\n",
      "Epoch 180 - Loss: 3.9473525386237918\n",
      "Epoch 181 - Loss: 3.9470392701100345\n",
      "Epoch 182 - Loss: 3.9467223242822875\n",
      "Epoch 183 - Loss: 3.946401636756691\n",
      "Epoch 184 - Loss: 3.9460771416519247\n",
      "Epoch 185 - Loss: 3.945748771545495\n",
      "Epoch 186 - Loss: 3.945416457428512\n",
      "Epoch 187 - Loss: 3.9450801286589026\n",
      "Epoch 188 - Loss: 3.944739712912979\n",
      "Epoch 189 - Loss: 3.9443951361353267\n",
      "Epoch 190 - Loss: 3.944046322486889\n",
      "Epoch 191 - Loss: 3.943693194291219\n",
      "Epoch 192 - Loss: 3.9433356719787955\n",
      "Epoch 193 - Loss: 3.9429736740293193\n",
      "Epoch 194 - Loss: 3.9426071169119195\n",
      "Epoch 195 - Loss: 3.9422359150231614\n",
      "Epoch 196 - Loss: 3.941859980622771\n",
      "Epoch 197 - Loss: 3.941479223766973\n",
      "Epoch 198 - Loss: 3.9410935522393444\n",
      "Epoch 199 - Loss: 3.940702871479059\n",
      "Epoch 200 - Loss: 3.940307084506432\n",
      "Epoch 201 - Loss: 3.939906091845616\n",
      "Epoch 202 - Loss: 3.9394997914443453\n",
      "Epoch 203 - Loss: 3.939088078590581\n",
      "Epoch 204 - Loss: 3.9386708458259316\n",
      "Epoch 205 - Loss: 3.938247982855679\n",
      "Epoch 206 - Loss: 3.9378193764552845\n",
      "Epoch 207 - Loss: 3.937384910373181\n",
      "Epoch 208 - Loss: 3.9369444652297148\n",
      "Epoch 209 - Loss: 3.93649791841203\n",
      "Epoch 210 - Loss: 3.9360451439647037\n",
      "Epoch 211 - Loss: 3.93558601247597\n",
      "Epoch 212 - Loss: 3.935120390959267\n",
      "Epoch 213 - Loss: 3.934648142729935\n",
      "Epoch 214 - Loss: 3.934169127276802\n",
      "Epoch 215 - Loss: 3.933683200128412\n",
      "Epoch 216 - Loss: 3.933190212713657\n",
      "Epoch 217 - Loss: 3.9326900122165083\n",
      "Epoch 218 - Loss: 3.9321824414245787\n",
      "Epoch 219 - Loss: 3.931667338571198\n",
      "Epoch 220 - Loss: 3.931144537170687\n",
      "Epoch 221 - Loss: 3.9306138658464627\n",
      "Epoch 222 - Loss: 3.9300751481516585\n",
      "Epoch 223 - Loss: 3.9295282023818237\n",
      "Epoch 224 - Loss: 3.928972841379327\n",
      "Epoch 225 - Loss: 3.92840887232904\n",
      "Epoch 226 - Loss: 3.9278360965448096\n",
      "Epoch 227 - Loss: 3.9272543092462833\n",
      "Epoch 228 - Loss: 3.9266632993255395\n",
      "Epoch 229 - Loss: 3.926062849102993\n",
      "Epoch 230 - Loss: 3.9254527340720125\n",
      "Epoch 231 - Loss: 3.9248327226316135\n",
      "Epoch 232 - Loss: 3.9242025758066035\n",
      "Epoch 233 - Loss: 3.923562046954474\n",
      "Epoch 234 - Loss: 3.9229108814583085\n",
      "Epoch 235 - Loss: 3.9222488164049407\n",
      "Epoch 236 - Loss: 3.9215755802475223\n",
      "Epoch 237 - Loss: 3.9208908924516126\n",
      "Epoch 238 - Loss: 3.920194463123877\n",
      "Epoch 239 - Loss: 3.919485992622365\n",
      "Epoch 240 - Loss: 3.918765171147327\n",
      "Epoch 241 - Loss: 3.9180316783114106\n",
      "Epoch 242 - Loss: 3.9172851826880457\n",
      "Epoch 243 - Loss: 3.916525341336714\n",
      "Epoch 244 - Loss: 3.9157517993037265\n",
      "Epoch 245 - Loss: 3.9149641890970166\n",
      "Epoch 246 - Loss: 3.9141621301334073\n",
      "Epoch 247 - Loss: 3.913345228156626\n",
      "Epoch 248 - Loss: 3.9125130746242864\n",
      "Epoch 249 - Loss: 3.9116652460618964\n",
      "Epoch 250 - Loss: 3.9108013033818265\n",
      "Epoch 251 - Loss: 3.909920791165008\n",
      "Epoch 252 - Loss: 3.909023236902995\n",
      "Epoch 253 - Loss: 3.9081081501978283\n",
      "Epoch 254 - Loss: 3.9071750219169585\n",
      "Epoch 255 - Loss: 3.9062233233002877\n",
      "Epoch 256 - Loss: 3.9052525050161524\n",
      "Epoch 257 - Loss: 3.904261996162856\n",
      "Epoch 258 - Loss: 3.9032512032120645\n",
      "Epoch 259 - Loss: 3.9022195088901386\n",
      "Epoch 260 - Loss: 3.9011662709931207\n",
      "Epoch 261 - Loss: 3.9000908211308083\n",
      "Epoch 262 - Loss: 3.8989924633949427\n",
      "Epoch 263 - Loss: 3.897870472946182\n",
      "Epoch 264 - Loss: 3.8967240945140538\n",
      "Epoch 265 - Loss: 3.8955525408036564\n",
      "Epoch 266 - Loss: 3.894354990802335\n",
      "Epoch 267 - Loss: 3.893130587979\n",
      "Epoch 268 - Loss: 3.891878438368156\n",
      "Epoch 269 - Loss: 3.890597608530032\n",
      "Epoch 270 - Loss: 3.889287123377443\n",
      "Epoch 271 - Loss: 3.8879459638592566\n",
      "Epoch 272 - Loss: 3.886573064489411\n",
      "Epoch 273 - Loss: 3.885167310709464\n",
      "Epoch 274 - Loss: 3.883727536071617\n",
      "Epoch 275 - Loss: 3.8822525192279453\n",
      "Epoch 276 - Loss: 3.8807409807102817\n",
      "Epoch 277 - Loss: 3.8791915794838054\n",
      "Epoch 278 - Loss: 3.8776029092557605\n",
      "Epoch 279 - Loss: 3.8759734945190445\n",
      "Epoch 280 - Loss: 3.8743017863084597\n",
      "Epoch 281 - Loss: 3.8725861576452982\n",
      "Epoch 282 - Loss: 3.8708248986435883\n",
      "Epoch 283 - Loss: 3.869016211248707\n",
      "Epoch 284 - Loss: 3.8671582035761927\n",
      "Epoch 285 - Loss: 3.8652488838153496\n",
      "Epoch 286 - Loss: 3.863286153658672\n",
      "Epoch 287 - Loss: 3.861267801214122\n",
      "Epoch 288 - Loss: 3.8591914933528733\n",
      "Epoch 289 - Loss: 3.8570547674401414\n",
      "Epoch 290 - Loss: 3.8548550223912152\n",
      "Epoch 291 - Loss: 3.8525895089885758\n",
      "Epoch 292 - Loss: 3.8502553193890763\n",
      "Epoch 293 - Loss: 3.847849375742323\n",
      "Epoch 294 - Loss: 3.8453684178327086\n",
      "Epoch 295 - Loss: 3.8428089896476703\n",
      "Epoch 296 - Loss: 3.840167424763684\n",
      "Epoch 297 - Loss: 3.8374398304290427\n",
      "Epoch 298 - Loss: 3.834622070208352\n",
      "Epoch 299 - Loss: 3.8317097450377515\n",
      "Epoch 300 - Loss: 3.828698172521872\n",
      "Epoch 301 - Loss: 3.825582364283031\n",
      "Epoch 302 - Loss: 3.822357001150044\n",
      "Epoch 303 - Loss: 3.8190164059475915\n",
      "Epoch 304 - Loss: 3.8155545136170863\n",
      "Epoch 305 - Loss: 3.811964838365731\n",
      "Epoch 306 - Loss: 3.8082404375013565\n",
      "Epoch 307 - Loss: 3.8043738715659465\n",
      "Epoch 308 - Loss: 3.800357160329505\n",
      "Epoch 309 - Loss: 3.7961817341472575\n",
      "Epoch 310 - Loss: 3.7918383801156277\n",
      "Epoch 311 - Loss: 3.7873171823848844\n",
      "Epoch 312 - Loss: 3.782607455896875\n",
      "Epoch 313 - Loss: 3.7776976727130482\n",
      "Epoch 314 - Loss: 3.772575379978736\n",
      "Epoch 315 - Loss: 3.7672271084315905\n",
      "Epoch 316 - Loss: 3.7616382702021376\n",
      "Epoch 317 - Loss: 3.7557930444687266\n",
      "Epoch 318 - Loss: 3.7496742493134505\n",
      "Epoch 319 - Loss: 3.743263197874659\n",
      "Epoch 320 - Loss: 3.736539536599382\n",
      "Epoch 321 - Loss: 3.7294810630582758\n",
      "Epoch 322 - Loss: 3.7220635203882284\n",
      "Epoch 323 - Loss: 3.7142603649637533\n",
      "Epoch 324 - Loss: 3.706042503356645\n",
      "Epoch 325 - Loss: 3.6973779940109073\n",
      "Epoch 326 - Loss: 3.6882317083220175\n",
      "Epoch 327 - Loss: 3.678564944949416\n",
      "Epoch 328 - Loss: 3.6683349901902798\n",
      "Epoch 329 - Loss: 3.6574946160812325\n",
      "Epoch 330 - Loss: 3.645991506552274\n",
      "Epoch 331 - Loss: 3.6337676004140804\n",
      "Epoch 332 - Loss: 3.620758338199753\n",
      "Epoch 333 - Loss: 3.6068917978961696\n",
      "Epoch 334 - Loss: 3.59208770239372\n",
      "Epoch 335 - Loss: 3.5762562790865964\n",
      "Epoch 336 - Loss: 3.559296949540123\n",
      "Epoch 337 - Loss: 3.5410968246445456\n",
      "Epoch 338 - Loss: 3.5215289784381842\n",
      "Epoch 339 - Loss: 3.500450472210748\n",
      "Epoch 340 - Loss: 3.477700100241015\n",
      "Epoch 341 - Loss: 3.4530958306061383\n",
      "Epoch 342 - Loss: 3.4264319205021843\n",
      "Epoch 343 - Loss: 3.3974756978332277\n",
      "Epoch 344 - Loss: 3.3659640230346324\n",
      "Epoch 345 - Loss: 3.331599482438187\n",
      "Epoch 346 - Loss: 3.294046424495835\n",
      "Epoch 347 - Loss: 3.252927043400558\n",
      "Epoch 348 - Loss: 3.2078178553199925\n",
      "Epoch 349 - Loss: 3.158247118828161\n",
      "Epoch 350 - Loss: 3.103694044569089\n",
      "Epoch 351 - Loss: 3.0435910407861706\n",
      "Epoch 352 - Loss: 2.9773307631993533\n",
      "Epoch 353 - Loss: 2.9042803650148126\n",
      "Epoch 354 - Loss: 2.8238060015272994\n",
      "Epoch 355 - Loss: 2.735311149352847\n",
      "Epoch 356 - Loss: 2.6382922916123666\n",
      "Epoch 357 - Loss: 2.5324144034204252\n",
      "Epoch 358 - Loss: 2.417605644384798\n",
      "Epoch 359 - Loss: 2.2941649515435225\n",
      "Epoch 360 - Loss: 2.1628676861423246\n",
      "Epoch 361 - Loss: 2.0250446105504496\n",
      "Epoch 362 - Loss: 1.88260231205926\n",
      "Epoch 363 - Loss: 1.7379550442181508\n",
      "Epoch 364 - Loss: 1.5938543500649351\n",
      "Epoch 365 - Loss: 1.4531330650917647\n",
      "Epoch 366 - Loss: 1.3184134875712228\n",
      "Epoch 367 - Loss: 1.1918480530526234\n",
      "Epoch 368 - Loss: 1.0749521650329634\n",
      "Epoch 369 - Loss: 0.9685561708496591\n",
      "Epoch 370 - Loss: 0.8728645390725107\n",
      "Epoch 371 - Loss: 0.7875840009139914\n",
      "Epoch 372 - Loss: 0.7120765297321213\n",
      "Epoch 373 - Loss: 0.6455028360125684\n",
      "Epoch 374 - Loss: 0.5869376385849873\n",
      "Epoch 375 - Loss: 0.535451525246522\n",
      "Epoch 376 - Loss: 0.49016259035611737\n",
      "Epoch 377 - Loss: 0.4502645666705374\n",
      "Epoch 378 - Loss: 0.4150385493832141\n",
      "Epoch 379 - Loss: 0.38385428513185893\n",
      "Epoch 380 - Loss: 0.35616545156814305\n",
      "Epoch 381 - Loss: 0.3315019287142897\n",
      "Epoch 382 - Loss: 0.30946095359748527\n",
      "Epoch 383 - Loss: 0.28969826587940767\n",
      "Epoch 384 - Loss: 0.27191983565804884\n",
      "Epoch 385 - Loss: 0.25587444307488105\n",
      "Epoch 386 - Loss: 0.24134719010125508\n",
      "Epoch 387 - Loss: 0.2281539207373432\n",
      "Epoch 388 - Loss: 0.21613647411577072\n",
      "Epoch 389 - Loss: 0.2051586741465816\n",
      "Epoch 390 - Loss: 0.195102955781637\n",
      "Epoch 391 - Loss: 0.18586753342215348\n",
      "Epoch 392 - Loss: 0.1773640265520061\n",
      "Epoch 393 - Loss: 0.1695154685803155\n",
      "Epoch 394 - Loss: 0.16225463563372436\n",
      "Epoch 395 - Loss: 0.15552264192427773\n",
      "Epoch 396 - Loss: 0.14926775703920636\n",
      "Epoch 397 - Loss: 0.14344440799780872\n",
      "Epoch 398 - Loss: 0.13801233526311582\n",
      "Epoch 399 - Loss: 0.13293587720186673\n",
      "Epoch 400 - Loss: 0.12818336189285612\n",
      "Epoch 401 - Loss: 0.12372658882627935\n",
      "Epoch 402 - Loss: 0.11954038603916972\n",
      "Epoch 403 - Loss: 0.11560223070317044\n",
      "Epoch 404 - Loss: 0.111891923213659\n",
      "Epoch 405 - Loss: 0.1083913065018088\n",
      "Epoch 406 - Loss: 0.10508402366843721\n",
      "Epoch 407 - Loss: 0.10195530817396412\n",
      "Epoch 408 - Loss: 0.09899180175638543\n",
      "Epoch 409 - Loss: 0.09618139602471576\n",
      "Epoch 410 - Loss: 0.09351309431813512\n",
      "Epoch 411 - Loss: 0.09097689095493891\n",
      "Epoch 412 - Loss: 0.0885636654397609\n",
      "Epoch 413 - Loss: 0.08626508956826112\n",
      "Epoch 414 - Loss: 0.08407354567846193\n",
      "Epoch 415 - Loss: 0.08198205455772478\n",
      "Epoch 416 - Loss: 0.07998421173263118\n",
      "Epoch 417 - Loss: 0.07807413105282336\n",
      "Epoch 418 - Loss: 0.0762463946349863\n",
      "Epoch 419 - Loss: 0.07449600836438497\n",
      "Epoch 420 - Loss: 0.07281836226264118\n",
      "Epoch 421 - Loss: 0.07120919512499617\n",
      "Epoch 422 - Loss: 0.06966456291083874\n",
      "Epoch 423 - Loss: 0.06818081044002532\n",
      "Epoch 424 - Loss: 0.06675454600631937\n",
      "Epoch 425 - Loss: 0.06538261856968308\n",
      "Epoch 426 - Loss: 0.06406209723245364\n",
      "Epoch 427 - Loss: 0.062790252741702\n",
      "Epoch 428 - Loss: 0.0615645407922142\n",
      "Epoch 429 - Loss: 0.060382586932307886\n",
      "Epoch 430 - Loss: 0.059242172898742985\n",
      "Epoch 431 - Loss: 0.05814122422784227\n",
      "Epoch 432 - Loss: 0.05707779900806161\n",
      "Epoch 433 - Loss: 0.05605007765503001\n",
      "Epoch 434 - Loss: 0.05505635360383584\n",
      "Epoch 435 - Loss: 0.05409502482536057\n",
      "Epoch 436 - Loss: 0.053164586083976506\n",
      "Epoch 437 - Loss: 0.05226362186314596\n",
      "Epoch 438 - Loss: 0.05139079989355424\n",
      "Epoch 439 - Loss: 0.05054486522552438\n",
      "Epoch 440 - Loss: 0.04972463479373163\n",
      "Epoch 441 - Loss: 0.04892899242776252\n",
      "Epoch 442 - Loss: 0.04815688426694917\n",
      "Epoch 443 - Loss: 0.04740731454222737\n",
      "Epoch 444 - Loss: 0.04667934169159658\n",
      "Epoch 445 - Loss: 0.04597207477915406\n",
      "Epoch 446 - Loss: 0.0452846701906925\n",
      "Epoch 447 - Loss: 0.04461632858153345\n",
      "Epoch 448 - Loss: 0.04396629205465935\n",
      "Epoch 449 - Loss: 0.043333841549338545\n",
      "Epoch 450 - Loss: 0.04271829442234227\n",
      "Epoch 451 - Loss: 0.04211900220555367\n",
      "Epoch 452 - Loss: 0.04153534852529391\n",
      "Epoch 453 - Loss: 0.04096674717005706\n",
      "Epoch 454 - Loss: 0.0404126402945694\n",
      "Epoch 455 - Loss: 0.03987249674919279\n",
      "Epoch 456 - Loss: 0.039345810524680426\n",
      "Epoch 457 - Loss: 0.038832099303186006\n",
      "Epoch 458 - Loss: 0.03833090310723269\n",
      "Epoch 459 - Loss: 0.03784178303907165\n",
      "Epoch 460 - Loss: 0.037364320103518554\n",
      "Epoch 461 - Loss: 0.03689811410794619\n",
      "Epoch 462 - Loss: 0.03644278263365051\n",
      "Epoch 463 - Loss: 0.035997960073293034\n",
      "Epoch 464 - Loss: 0.035563296729562856\n",
      "Epoch 465 - Loss: 0.03513845797060355\n",
      "Epoch 466 - Loss: 0.03472312343811272\n",
      "Epoch 467 - Loss: 0.034316986304354674\n",
      "Epoch 468 - Loss: 0.03391975257462665\n",
      "Epoch 469 - Loss: 0.03353114043199462\n",
      "Epoch 470 - Loss: 0.033150879621365044\n",
      "Epoch 471 - Loss: 0.032778710870188195\n",
      "Epoch 472 - Loss: 0.032414385343296645\n",
      "Epoch 473 - Loss: 0.032057664129574275\n",
      "Epoch 474 - Loss: 0.031708317758326186\n",
      "Epoch 475 - Loss: 0.03136612574338045\n",
      "Epoch 476 - Loss: 0.03103087615309755\n",
      "Epoch 477 - Loss: 0.030702365204601203\n",
      "Epoch 478 - Loss: 0.030380396880664864\n",
      "Epoch 479 - Loss: 0.03006478256780415\n",
      "Epoch 480 - Loss: 0.02975534071422878\n",
      "Epoch 481 - Loss: 0.029451896506403733\n",
      "Epoch 482 - Loss: 0.029154281563058288\n",
      "Epoch 483 - Loss: 0.028862333645563534\n",
      "Epoch 484 - Loss: 0.028575896383672823\n",
      "Epoch 485 - Loss: 0.028294819015690444\n",
      "Epoch 486 - Loss: 0.028018956142197477\n",
      "Epoch 487 - Loss: 0.027748167492522464\n",
      "Epoch 488 - Loss: 0.027482317703199862\n",
      "Epoch 489 - Loss: 0.027221276107710104\n",
      "Epoch 490 - Loss: 0.026964916536841015\n",
      "Epoch 491 - Loss: 0.026713117129054883\n",
      "Epoch 492 - Loss: 0.02646576015028501\n",
      "Epoch 493 - Loss: 0.026222731822623635\n",
      "Epoch 494 - Loss: 0.025983922161396278\n",
      "Epoch 495 - Loss: 0.025749224820151986\n",
      "Epoch 496 - Loss: 0.025518536943126998\n",
      "Epoch 497 - Loss: 0.025291759024767678\n",
      "Epoch 498 - Loss: 0.02506879477592487\n",
      "Epoch 499 - Loss: 0.024849550996355572\n",
      "Epoch 500 - Loss: 0.024633937453189817\n",
      "Epoch 501 - Loss: 0.024421866765042134\n",
      "Epoch 502 - Loss: 0.02421325429146652\n",
      "Epoch 503 - Loss: 0.024008018027471118\n",
      "Epoch 504 - Loss: 0.0238060785028264\n",
      "Epoch 505 - Loss: 0.023607358685917013\n",
      "Epoch 506 - Loss: 0.023411783891900392\n",
      "Epoch 507 - Loss: 0.023219281694951337\n",
      "Epoch 508 - Loss: 0.023029781844382716\n",
      "Epoch 509 - Loss: 0.022843216184445528\n",
      "Epoch 510 - Loss: 0.02265951857762314\n",
      "Epoch 511 - Loss: 0.022478624831243726\n",
      "Epoch 512 - Loss: 0.022300472627246163\n",
      "Epoch 513 - Loss: 0.022125001454943703\n",
      "Epoch 514 - Loss: 0.021952152546637887\n",
      "Epoch 515 - Loss: 0.021781868815943236\n",
      "Epoch 516 - Loss: 0.0216140947986923\n",
      "Epoch 517 - Loss: 0.02144877659629597\n",
      "Epoch 518 - Loss: 0.021285861821442098\n",
      "Epoch 519 - Loss: 0.021125299546020902\n",
      "Epoch 520 - Loss: 0.02096704025117235\n",
      "Epoch 521 - Loss: 0.020811035779356005\n",
      "Epoch 522 - Loss: 0.020657239288348457\n",
      "Epoch 523 - Loss: 0.020505605207079964\n",
      "Epoch 524 - Loss: 0.020356089193224607\n",
      "Epoch 525 - Loss: 0.020208648092464872\n",
      "Epoch 526 - Loss: 0.020063239899353418\n",
      "Epoch 527 - Loss: 0.01991982371970061\n",
      "Epoch 528 - Loss: 0.019778359734419257\n",
      "Epoch 529 - Loss: 0.019638809164760976\n",
      "Epoch 530 - Loss: 0.019501134238883066\n",
      "Epoch 531 - Loss: 0.019365298159687024\n",
      "Epoch 532 - Loss: 0.019231265073872746\n",
      "Epoch 533 - Loss: 0.0190990000421557\n",
      "Epoch 534 - Loss: 0.018968469010596876\n",
      "Epoch 535 - Loss: 0.018839638782996722\n",
      "Epoch 536 - Loss: 0.01871247699430904\n",
      "Epoch 537 - Loss: 0.018586952085029855\n",
      "Epoch 538 - Loss: 0.018463033276521053\n",
      "Epoch 539 - Loss: 0.018340690547229047\n",
      "Epoch 540 - Loss: 0.0182198946097612\n",
      "Epoch 541 - Loss: 0.018100616888784213\n",
      "Epoch 542 - Loss: 0.017982829499710525\n",
      "Epoch 543 - Loss: 0.01786650522814042\n",
      "Epoch 544 - Loss: 0.017751617510028916\n",
      "Epoch 545 - Loss: 0.017638140412547604\n",
      "Epoch 546 - Loss: 0.01752604861561426\n",
      "Epoch 547 - Loss: 0.01741531739406209\n",
      "Epoch 548 - Loss: 0.017305922600424403\n",
      "Epoch 549 - Loss: 0.01719784064830894\n",
      "Epoch 550 - Loss: 0.01709104849633959\n",
      "Epoch 551 - Loss: 0.016985523632642635\n",
      "Epoch 552 - Loss: 0.016881244059856138\n",
      "Epoch 553 - Loss: 0.016778188280642536\n",
      "Epoch 554 - Loss: 0.016676335283684587\n",
      "Epoch 555 - Loss: 0.016575664530146215\n",
      "Epoch 556 - Loss: 0.01647615594058043\n",
      "Epoch 557 - Loss: 0.016377789882267322\n",
      "Epoch 558 - Loss: 0.01628054715696555\n",
      "Epoch 559 - Loss: 0.016184408989062465\n",
      "Epoch 560 - Loss: 0.01608935701410669\n",
      "Epoch 561 - Loss: 0.01599537326771041\n",
      "Epoch 562 - Loss: 0.015902440174806114\n",
      "Epoch 563 - Loss: 0.015810540539245962\n",
      "Epoch 564 - Loss: 0.015719657533730353\n",
      "Epoch 565 - Loss: 0.015629774690053926\n",
      "Epoch 566 - Loss: 0.01554087588965763\n",
      "Epoch 567 - Loss: 0.015452945354475582\n",
      "Epoch 568 - Loss: 0.015365967638065959\n",
      "Epoch 569 - Loss: 0.015279927617016046\n",
      "Epoch 570 - Loss: 0.015194810482611473\n",
      "Epoch 571 - Loss: 0.015110601732760376\n",
      "Epoch 572 - Loss: 0.015027287164163296\n",
      "Epoch 573 - Loss: 0.01494485286472035\n",
      "Epoch 574 - Loss: 0.014863285206167249\n",
      "Epoch 575 - Loss: 0.01478257083693241\n",
      "Epoch 576 - Loss: 0.014702696675207249\n",
      "Epoch 577 - Loss: 0.014623649902222266\n",
      "Epoch 578 - Loss: 0.014545417955722296\n",
      "Epoch 579 - Loss: 0.014467988523633492\n",
      "Epoch 580 - Loss: 0.01439134953791615\n",
      "Epoch 581 - Loss: 0.014315489168596604\n",
      "Epoch 582 - Loss: 0.014240395817972314\n",
      "Epoch 583 - Loss: 0.014166058114984528\n",
      "Epoch 584 - Loss: 0.014092464909752618\n",
      "Epoch 585 - Loss: 0.01401960526826487\n",
      "Epoch 586 - Loss: 0.013947468467220678\n",
      "Epoch 587 - Loss: 0.013876043989018932\n",
      "Epoch 588 - Loss: 0.013805321516887792\n",
      "Epoch 589 - Loss: 0.013735290930151611\n",
      "Epoch 590 - Loss: 0.013665942299630076\n",
      "Epoch 591 - Loss: 0.013597265883165491\n",
      "Epoch 592 - Loss: 0.01352925212127436\n",
      "Epoch 593 - Loss: 0.013461891632918568\n",
      "Epoch 594 - Loss: 0.013395175211393365\n",
      "Epoch 595 - Loss: 0.013329093820327216\n",
      "Epoch 596 - Loss: 0.013263638589791191\n",
      "Epoch 597 - Loss: 0.013198800812513626\n",
      "Epoch 598 - Loss: 0.01313457194019702\n",
      "Epoch 599 - Loss: 0.01307094357993403\n",
      "Epoch 600 - Loss: 0.013007907490719365\n",
      "Epoch 601 - Loss: 0.012945455580054836\n",
      "Epoch 602 - Loss: 0.012883579900644183\n",
      "Epoch 603 - Loss: 0.012822272647175748\n",
      "Epoch 604 - Loss: 0.01276152615318932\n",
      "Epoch 605 - Loss: 0.012701332888025493\n",
      "Epoch 606 - Loss: 0.012641685453854563\n",
      "Epoch 607 - Loss: 0.012582576582782274\n",
      "Epoch 608 - Loss: 0.012523999134031166\n",
      "Epoch 609 - Loss: 0.012465946091193883\n",
      "Epoch 610 - Loss: 0.01240841055955738\n",
      "Epoch 611 - Loss: 0.012351385763495502\n",
      "Epoch 612 - Loss: 0.012294865043927754\n",
      "Epoch 613 - Loss: 0.012238841855842623\n",
      "Epoch 614 - Loss: 0.012183309765883513\n",
      "Epoch 615 - Loss: 0.012128262449995186\n",
      "Epoch 616 - Loss: 0.012073693691129313\n",
      "Epoch 617 - Loss: 0.01201959737700723\n",
      "Epoch 618 - Loss: 0.011965967497938157\n",
      "Epoch 619 - Loss: 0.011912798144691664\n",
      "Epoch 620 - Loss: 0.011860083506422243\n",
      "Epoch 621 - Loss: 0.011807817868645156\n",
      "Epoch 622 - Loss: 0.011755995611261545\n",
      "Epoch 623 - Loss: 0.011704611206631698\n",
      "Epoch 624 - Loss: 0.011653659217695232\n",
      "Epoch 625 - Loss: 0.011603134296136387\n",
      "Epoch 626 - Loss: 0.011553031180593804\n",
      "Epoch 627 - Loss: 0.01150334469491303\n",
      "Epoch 628 - Loss: 0.011454069746440816\n",
      "Epoch 629 - Loss: 0.011405201324360128\n",
      "Epoch 630 - Loss: 0.011356734498064356\n",
      "Epoch 631 - Loss: 0.011308664415570152\n",
      "Epoch 632 - Loss: 0.011260986301967552\n",
      "Epoch 633 - Loss: 0.01121369545790642\n",
      "Epoch 634 - Loss: 0.011166787258118076\n",
      "Epoch 635 - Loss: 0.011120257149971577\n",
      "Epoch 636 - Loss: 0.011074100652063249\n",
      "Epoch 637 - Loss: 0.011028313352838893\n",
      "Epoch 638 - Loss: 0.0109828909092475\n",
      "Epoch 639 - Loss: 0.01093782904542623\n",
      "Epoch 640 - Loss: 0.010893123551414821\n",
      "Epoch 641 - Loss: 0.010848770281899739\n",
      "Epoch 642 - Loss: 0.010804765154986625\n",
      "Epoch 643 - Loss: 0.01076110415100036\n",
      "Epoch 644 - Loss: 0.01071778331131216\n",
      "Epoch 645 - Loss: 0.010674798737193198\n",
      "Epoch 646 - Loss: 0.010632146588693481\n",
      "Epoch 647 - Loss: 0.010589823083545957\n",
      "Epoch 648 - Loss: 0.010547824496094703\n",
      "Epoch 649 - Loss: 0.01050614715624701\n",
      "Epoch 650 - Loss: 0.010464787448448384\n",
      "Epoch 651 - Loss: 0.010423741810680093\n",
      "Epoch 652 - Loss: 0.010383006733478672\n",
      "Epoch 653 - Loss: 0.01034257875897665\n",
      "Epoch 654 - Loss: 0.010302454479964291\n",
      "Epoch 655 - Loss: 0.010262630538971506\n",
      "Epoch 656 - Loss: 0.010223103627369563\n",
      "Epoch 657 - Loss: 0.010183870484492114\n",
      "Epoch 658 - Loss: 0.010144927896775093\n",
      "Epoch 659 - Loss: 0.010106272696914905\n",
      "Epoch 660 - Loss: 0.010067901763044505\n",
      "Epoch 661 - Loss: 0.010029812017926885\n",
      "Epoch 662 - Loss: 0.009992000428165808\n",
      "Epoch 663 - Loss: 0.009954464003432898\n",
      "Epoch 664 - Loss: 0.009917199795711061\n",
      "Epoch 665 - Loss: 0.009880204898553702\n",
      "Epoch 666 - Loss: 0.00984347644635939\n",
      "Epoch 667 - Loss: 0.009807011613661537\n",
      "Epoch 668 - Loss: 0.009770807614432856\n",
      "Epoch 669 - Loss: 0.009734861701404035\n",
      "Epoch 670 - Loss: 0.009699171165396566\n",
      "Epoch 671 - Loss: 0.009663733334669062\n",
      "Epoch 672 - Loss: 0.00962854557427717\n",
      "Epoch 673 - Loss: 0.009593605285446196\n",
      "Epoch 674 - Loss: 0.009558909904956803\n",
      "Epoch 675 - Loss: 0.009524456904542726\n",
      "Epoch 676 - Loss: 0.009490243790301101\n",
      "Epoch 677 - Loss: 0.009456268102114165\n",
      "Epoch 678 - Loss: 0.009422527413082882\n",
      "Epoch 679 - Loss: 0.009389019328971676\n",
      "Epoch 680 - Loss: 0.009355741487664332\n",
      "Epoch 681 - Loss: 0.00932269155863058\n",
      "Epoch 682 - Loss: 0.009289867242403299\n",
      "Epoch 683 - Loss: 0.009257266270065912\n",
      "Epoch 684 - Loss: 0.009224886402750081\n",
      "Epoch 685 - Loss: 0.00919272543114294\n",
      "Epoch 686 - Loss: 0.009160781175004153\n",
      "Epoch 687 - Loss: 0.009129051482692285\n",
      "Epoch 688 - Loss: 0.009097534230700371\n",
      "Epoch 689 - Loss: 0.009066227323200433\n",
      "Epoch 690 - Loss: 0.009035128691596887\n",
      "Epoch 691 - Loss: 0.009004236294088318\n",
      "Epoch 692 - Loss: 0.008973548115237942\n",
      "Epoch 693 - Loss: 0.008943062165551888\n",
      "Epoch 694 - Loss: 0.008912776481065976\n",
      "Epoch 695 - Loss: 0.008882689122939862\n",
      "Epoch 696 - Loss: 0.008852798177059171\n",
      "Epoch 697 - Loss: 0.008823101753645065\n",
      "Epoch 698 - Loss: 0.008793597986871068\n",
      "Epoch 699 - Loss: 0.00876428503448716\n",
      "Epoch 700 - Loss: 0.008735161077450872\n",
      "Epoch 701 - Loss: 0.008706224319565222\n",
      "Epoch 702 - Loss: 0.008677472987123482\n",
      "Epoch 703 - Loss: 0.008648905328560288\n",
      "Epoch 704 - Loss: 0.008620519614109527\n",
      "Epoch 705 - Loss: 0.008592314135468212\n",
      "Epoch 706 - Loss: 0.008564287205466757\n",
      "Epoch 707 - Loss: 0.008536437157745045\n",
      "Epoch 708 - Loss: 0.008508762346434674\n",
      "Epoch 709 - Loss: 0.008481261145846845\n",
      "Epoch 710 - Loss: 0.008453931950165843\n",
      "Epoch 711 - Loss: 0.008426773173148178\n",
      "Epoch 712 - Loss: 0.008399783247827123\n",
      "Epoch 713 - Loss: 0.008372960626222553\n",
      "Epoch 714 - Loss: 0.008346303779055934\n",
      "Epoch 715 - Loss: 0.008319811195470555\n",
      "Epoch 716 - Loss: 0.008293481382756512\n",
      "Epoch 717 - Loss: 0.008267312866080834\n",
      "Epoch 718 - Loss: 0.008241304188222236\n",
      "Epoch 719 - Loss: 0.008215453909310529\n",
      "Epoch 720 - Loss: 0.00818976060657077\n",
      "Epoch 721 - Loss: 0.008164222874071814\n",
      "Epoch 722 - Loss: 0.008138839322479207\n",
      "Epoch 723 - Loss: 0.008113608578812591\n",
      "Epoch 724 - Loss: 0.00808852928620703\n",
      "Epoch 725 - Loss: 0.00806360010367889\n",
      "Epoch 726 - Loss: 0.00803881970589536\n",
      "Epoch 727 - Loss: 0.008014186782948206\n",
      "Epoch 728 - Loss: 0.007989700040131418\n",
      "Epoch 729 - Loss: 0.007965358197722499\n",
      "Epoch 730 - Loss: 0.007941159990767745\n",
      "Epoch 731 - Loss: 0.007917104168870901\n",
      "Epoch 732 - Loss: 0.007893189495985808\n",
      "Epoch 733 - Loss: 0.007869414750212083\n",
      "Epoch 734 - Loss: 0.007845778723594727\n",
      "Epoch 735 - Loss: 0.007822280221926796\n",
      "Epoch 736 - Loss: 0.00779891806455563\n",
      "Epoch 737 - Loss: 0.007775691084192049\n",
      "Epoch 738 - Loss: 0.007752598126723066\n",
      "Epoch 739 - Loss: 0.007729638051027514\n",
      "Epoch 740 - Loss: 0.007706809728794817\n",
      "Epoch 741 - Loss: 0.007684112044346826\n",
      "Epoch 742 - Loss: 0.007661543894462456\n",
      "Epoch 743 - Loss: 0.007639104188205362\n",
      "Epoch 744 - Loss: 0.007616791846754449\n",
      "Epoch 745 - Loss: 0.007594605803237032\n",
      "Epoch 746 - Loss: 0.00757254500256497\n",
      "Epoch 747 - Loss: 0.007550608401273193\n",
      "Epoch 748 - Loss: 0.007528794967361095\n",
      "Epoch 749 - Loss: 0.007507103680136406\n",
      "Epoch 750 - Loss: 0.007485533530061634\n",
      "Epoch 751 - Loss: 0.007464083518602912\n",
      "Epoch 752 - Loss: 0.007442752658081404\n",
      "Epoch 753 - Loss: 0.007421539971526994\n",
      "Epoch 754 - Loss: 0.0074004444925344125\n",
      "Epoch 755 - Loss: 0.007379465265121602\n",
      "Epoch 756 - Loss: 0.007358601343590371\n",
      "Epoch 757 - Loss: 0.007337851792389297\n",
      "Epoch 758 - Loss: 0.007317215685978754\n",
      "Epoch 759 - Loss: 0.007296692108698078\n",
      "Epoch 760 - Loss: 0.007276280154634913\n",
      "Epoch 761 - Loss: 0.007255978927496481\n",
      "Epoch 762 - Loss: 0.007235787540483061\n",
      "Epoch 763 - Loss: 0.007215705116163285\n",
      "Epoch 764 - Loss: 0.007195730786351446\n",
      "Epoch 765 - Loss: 0.007175863691986705\n",
      "Epoch 766 - Loss: 0.007156102983014278\n",
      "Epoch 767 - Loss: 0.007136447818268344\n",
      "Epoch 768 - Loss: 0.007116897365356754\n",
      "Epoch 769 - Loss: 0.007097450800547676\n",
      "Epoch 770 - Loss: 0.007078107308657795\n",
      "Epoch 771 - Loss: 0.007058866082942395\n",
      "Epoch 772 - Loss: 0.007039726324987043\n",
      "Epoch 773 - Loss: 0.007020687244600908\n",
      "Epoch 774 - Loss: 0.0070017480597117715\n",
      "Epoch 775 - Loss: 0.006982907996262601\n",
      "Epoch 776 - Loss: 0.006964166288109719\n",
      "Epoch 777 - Loss: 0.006945522176922463\n",
      "Epoch 778 - Loss: 0.006926974912084403\n",
      "Epoch 779 - Loss: 0.006908523750596101\n",
      "Epoch 780 - Loss: 0.006890167956979145\n",
      "Epoch 781 - Loss: 0.006871906803181858\n",
      "Epoch 782 - Loss: 0.0068537395684862825\n",
      "Epoch 783 - Loss: 0.0068356655394165656\n",
      "Epoch 784 - Loss: 0.006817684009648677\n",
      "Epoch 785 - Loss: 0.0067997942799216116\n",
      "Epoch 786 - Loss: 0.006781995657949722\n",
      "Epoch 787 - Loss: 0.006764287458336507\n",
      "Epoch 788 - Loss: 0.006746669002489481\n",
      "Epoch 789 - Loss: 0.0067291396185365955\n",
      "Epoch 790 - Loss: 0.00671169864124349\n",
      "Epoch 791 - Loss: 0.0066943454119323485\n",
      "Epoch 792 - Loss: 0.006677079278401663\n",
      "Epoch 793 - Loss: 0.006659899594847305\n",
      "Epoch 794 - Loss: 0.006642805721784645\n",
      "Epoch 795 - Loss: 0.006625797025971967\n",
      "Epoch 796 - Loss: 0.006608872880334761\n",
      "Epoch 797 - Loss: 0.00659203266389133\n",
      "Epoch 798 - Loss: 0.00657527576167934\n",
      "Epoch 799 - Loss: 0.0065586015646833945\n",
      "Epoch 800 - Loss: 0.0065420094697638395\n",
      "Epoch 801 - Loss: 0.00652549887958629\n",
      "Epoch 802 - Loss: 0.0065090692025524775\n",
      "Epoch 803 - Loss: 0.006492719852731825\n",
      "Epoch 804 - Loss: 0.006476450249794158\n",
      "Epoch 805 - Loss: 0.0064602598189433\n",
      "Epoch 806 - Loss: 0.006444147990851571\n",
      "Epoch 807 - Loss: 0.006428114201595368\n",
      "Epoch 808 - Loss: 0.006412157892591397\n",
      "Epoch 809 - Loss: 0.006396278510534033\n",
      "Epoch 810 - Loss: 0.0063804755073334565\n",
      "Epoch 811 - Loss: 0.006364748340054644\n",
      "Epoch 812 - Loss: 0.006349096470857277\n",
      "Epoch 813 - Loss: 0.00633351936693633\n",
      "Epoch 814 - Loss: 0.006318016500463758\n",
      "Epoch 815 - Loss: 0.006302587348530653\n",
      "Epoch 816 - Loss: 0.006287231393090551\n",
      "Epoch 817 - Loss: 0.006271948120903182\n",
      "Epoch 818 - Loss: 0.006256737023479231\n",
      "Epoch 819 - Loss: 0.006241597597025792\n",
      "Epoch 820 - Loss: 0.006226529342392554\n",
      "Epoch 821 - Loss: 0.006211531765018703\n",
      "Epoch 822 - Loss: 0.006196604374880539\n",
      "Epoch 823 - Loss: 0.006181746686439933\n",
      "Epoch 824 - Loss: 0.006166958218593333\n",
      "Epoch 825 - Loss: 0.006152238494621479\n",
      "Epoch 826 - Loss: 0.0061375870421399225\n",
      "Epoch 827 - Loss: 0.006123003393050043\n",
      "Epoch 828 - Loss: 0.006108487083490848\n",
      "Epoch 829 - Loss: 0.0060940376537913935\n",
      "Epoch 830 - Loss: 0.0060796546484237655\n",
      "Epoch 831 - Loss: 0.006065337615956797\n",
      "Epoch 832 - Loss: 0.006051086109010391\n",
      "Epoch 833 - Loss: 0.006036899684210266\n",
      "Epoch 834 - Loss: 0.00602277790214368\n",
      "Epoch 835 - Loss: 0.006008720327315315\n",
      "Epoch 836 - Loss: 0.005994726528104025\n",
      "Epoch 837 - Loss: 0.00598079607672012\n",
      "Epoch 838 - Loss: 0.005966928549163048\n",
      "Epoch 839 - Loss: 0.005953123525179841\n",
      "Epoch 840 - Loss: 0.005939380588224012\n",
      "Epoch 841 - Loss: 0.005925699325414927\n",
      "Epoch 842 - Loss: 0.005912079327497859\n",
      "Epoch 843 - Loss: 0.005898520188804422\n",
      "Epoch 844 - Loss: 0.005885021507213568\n",
      "Epoch 845 - Loss: 0.005871582884113118\n",
      "Epoch 846 - Loss: 0.005858203924361781\n",
      "Epoch 847 - Loss: 0.005844884236251625\n",
      "Epoch 848 - Loss: 0.0058316234314710665\n",
      "Epoch 849 - Loss: 0.005818421125068318\n",
      "Epoch 850 - Loss: 0.005805276935415346\n",
      "Epoch 851 - Loss: 0.005792190484172195\n",
      "Epoch 852 - Loss: 0.005779161396251916\n",
      "Epoch 853 - Loss: 0.005766189299785792\n",
      "Epoch 854 - Loss: 0.005753273826089084\n",
      "Epoch 855 - Loss: 0.00574041460962722\n",
      "Epoch 856 - Loss: 0.005727611287982338\n",
      "Epoch 857 - Loss: 0.005714863501820439\n",
      "Epoch 858 - Loss: 0.005702170894858642\n",
      "Epoch 859 - Loss: 0.005689533113833146\n",
      "Epoch 860 - Loss: 0.0056769498084674715\n",
      "Epoch 861 - Loss: 0.0056644206314410556\n",
      "Epoch 862 - Loss: 0.00565194523835835\n",
      "Epoch 863 - Loss: 0.00563952328771824\n",
      "Epoch 864 - Loss: 0.005627154440883798\n",
      "Epoch 865 - Loss: 0.005614838362052568\n",
      "Epoch 866 - Loss: 0.005602574718227004\n",
      "Epoch 867 - Loss: 0.005590363179185494\n",
      "Epoch 868 - Loss: 0.005578203417453573\n",
      "Epoch 869 - Loss: 0.005566095108275588\n",
      "Epoch 870 - Loss: 0.005554037929586656\n",
      "Epoch 871 - Loss: 0.005542031561985043\n",
      "Epoch 872 - Loss: 0.005530075688704766\n",
      "Epoch 873 - Loss: 0.005518169995588645\n",
      "Epoch 874 - Loss: 0.005506314171061598\n",
      "Epoch 875 - Loss: 0.005494507906104352\n",
      "Epoch 876 - Loss: 0.0054827508942273465\n",
      "Epoch 877 - Loss: 0.005471042831445097\n",
      "Epoch 878 - Loss: 0.005459383416250765\n",
      "Epoch 879 - Loss: 0.005447772349591054\n",
      "Epoch 880 - Loss: 0.005436209334841486\n",
      "Epoch 881 - Loss: 0.005424694077781834\n",
      "Epoch 882 - Loss: 0.005413226286572046\n",
      "Epoch 883 - Loss: 0.005401805671728189\n",
      "Epoch 884 - Loss: 0.005390431946098984\n",
      "Epoch 885 - Loss: 0.0053791048248424\n",
      "Epoch 886 - Loss: 0.005367824025402592\n",
      "Epoch 887 - Loss: 0.0053565892674871645\n",
      "Epoch 888 - Loss: 0.005345400273044615\n",
      "Epoch 889 - Loss: 0.005334256766242151\n",
      "Epoch 890 - Loss: 0.005323158473443656\n",
      "Epoch 891 - Loss: 0.005312105123188037\n",
      "Epoch 892 - Loss: 0.005301096446167656\n",
      "Epoch 893 - Loss: 0.005290132175207272\n",
      "Epoch 894 - Loss: 0.005279212045242929\n",
      "Epoch 895 - Loss: 0.005268335793301353\n",
      "Epoch 896 - Loss: 0.005257503158479418\n",
      "Epoch 897 - Loss: 0.005246713881923949\n",
      "Epoch 898 - Loss: 0.005235967706811704\n",
      "Epoch 899 - Loss: 0.005225264378329626\n",
      "Epoch 900 - Loss: 0.005214603643655309\n",
      "Epoch 901 - Loss: 0.005203985251937677\n",
      "Epoch 902 - Loss: 0.005193408954277895\n",
      "Epoch 903 - Loss: 0.005182874503710535\n",
      "Epoch 904 - Loss: 0.005172381655184954\n",
      "Epoch 905 - Loss: 0.005161930165546737\n",
      "Epoch 906 - Loss: 0.005151519793519629\n",
      "Epoch 907 - Loss: 0.005141150299687425\n",
      "Epoch 908 - Loss: 0.005130821446476236\n",
      "Epoch 909 - Loss: 0.005120532998136824\n",
      "Epoch 910 - Loss: 0.005110284720727238\n",
      "Epoch 911 - Loss: 0.005100076382095596\n",
      "Epoch 912 - Loss: 0.0050899077518630955\n",
      "Epoch 913 - Loss: 0.005079778601407195\n",
      "Epoch 914 - Loss: 0.005069688703844963\n",
      "Epoch 915 - Loss: 0.0050596378340167026\n",
      "Epoch 916 - Loss: 0.005049625768469597\n",
      "Epoch 917 - Loss: 0.005039652285441781\n",
      "Epoch 918 - Loss: 0.005029717164846312\n",
      "Epoch 919 - Loss: 0.005019820188255599\n",
      "Epoch 920 - Loss: 0.005009961138885746\n",
      "Epoch 921 - Loss: 0.005000139801581239\n",
      "Epoch 922 - Loss: 0.004990355962799819\n",
      "Epoch 923 - Loss: 0.004980609410597348\n",
      "Epoch 924 - Loss: 0.004970899934613044\n",
      "Epoch 925 - Loss: 0.004961227326054757\n",
      "Epoch 926 - Loss: 0.004951591377684476\n",
      "Epoch 927 - Loss: 0.004941991883803896\n",
      "Epoch 928 - Loss: 0.004932428640240297\n",
      "Epoch 929 - Loss: 0.004922901444332425\n",
      "Epoch 930 - Loss: 0.00491341009491664\n",
      "Epoch 931 - Loss: 0.004903954392313163\n",
      "Epoch 932 - Loss: 0.004894534138312456\n",
      "Epoch 933 - Loss: 0.004885149136161825\n",
      "Epoch 934 - Loss: 0.004875799190552111\n",
      "Epoch 935 - Loss: 0.004866484107604479\n",
      "Epoch 936 - Loss: 0.004857203694857498\n",
      "Epoch 937 - Loss: 0.004847957761254237\n",
      "Epoch 938 - Loss: 0.004838746117129471\n",
      "Epoch 939 - Loss: 0.004829568574197225\n",
      "Epoch 940 - Loss: 0.004820424945538211\n",
      "Epoch 941 - Loss: 0.004811315045587544\n",
      "Epoch 942 - Loss: 0.004802238690122532\n",
      "Epoch 943 - Loss: 0.004793195696250678\n",
      "Epoch 944 - Loss: 0.004784185882397679\n",
      "Epoch 945 - Loss: 0.004775209068295685\n",
      "Epoch 946 - Loss: 0.004766265074971598\n",
      "Epoch 947 - Loss: 0.004757353724735487\n",
      "Epoch 948 - Loss: 0.0047484748411692635\n",
      "Epoch 949 - Loss: 0.0047396282491152415\n",
      "Epoch 950 - Loss: 0.004730813774665078\n",
      "Epoch 951 - Loss: 0.004722031245148625\n",
      "Epoch 952 - Loss: 0.004713280489123003\n",
      "Epoch 953 - Loss: 0.004704561336361801\n",
      "Epoch 954 - Loss: 0.004695873617844295\n",
      "Epoch 955 - Loss: 0.004687217165744877\n",
      "Epoch 956 - Loss: 0.004678591813422573\n",
      "Epoch 957 - Loss: 0.004669997395410601\n",
      "Epoch 958 - Loss: 0.004661433747406191\n",
      "Epoch 959 - Loss: 0.004652900706260295\n",
      "Epoch 960 - Loss: 0.004644398109967635\n",
      "Epoch 961 - Loss: 0.004635925797656651\n",
      "Epoch 962 - Loss: 0.0046274836095797234\n",
      "Epoch 963 - Loss: 0.004619071387103365\n",
      "Epoch 964 - Loss: 0.004610688972698594\n",
      "Epoch 965 - Loss: 0.004602336209931401\n",
      "Epoch 966 - Loss: 0.004594012943453228\n",
      "Epoch 967 - Loss: 0.004585719018991699\n",
      "Epoch 968 - Loss: 0.0045774542833413\n",
      "Epoch 969 - Loss: 0.0045692185843542316\n",
      "Epoch 970 - Loss: 0.004561011770931355\n",
      "Epoch 971 - Loss: 0.004552833693013173\n",
      "Epoch 972 - Loss: 0.004544684201570978\n",
      "Epoch 973 - Loss: 0.00453656314859806\n",
      "Epoch 974 - Loss: 0.004528470387100985\n",
      "Epoch 975 - Loss: 0.004520405771090991\n",
      "Epoch 976 - Loss: 0.004512369155575417\n",
      "Epoch 977 - Loss: 0.0045043603965493305\n",
      "Epoch 978 - Loss: 0.004496379350987095\n",
      "Epoch 979 - Loss: 0.004488425876834181\n",
      "Epoch 980 - Loss: 0.004480499832998885\n",
      "Epoch 981 - Loss: 0.004472601079344272\n",
      "Epoch 982 - Loss: 0.004464729476680168\n",
      "Epoch 983 - Loss: 0.0044568848867551775\n",
      "Epoch 984 - Loss: 0.004449067172248804\n",
      "Epoch 985 - Loss: 0.004441276196763722\n",
      "Epoch 986 - Loss: 0.004433511824818029\n",
      "Epoch 987 - Loss: 0.004425773921837623\n",
      "Epoch 988 - Loss: 0.004418062354148586\n",
      "Epoch 989 - Loss: 0.0044103769889698476\n",
      "Epoch 990 - Loss: 0.00440271769440563\n",
      "Epoch 991 - Loss: 0.004395084339438179\n",
      "Epoch 992 - Loss: 0.004387476793920525\n",
      "Epoch 993 - Loss: 0.004379894928569207\n",
      "Epoch 994 - Loss: 0.004372338614957285\n",
      "Epoch 995 - Loss: 0.0043648077255071605\n",
      "Epoch 996 - Loss: 0.004357302133483687\n",
      "Epoch 997 - Loss: 0.00434982171298722\n",
      "Epoch 998 - Loss: 0.004342366338946761\n",
      "Epoch 999 - Loss: 0.004334935887113248\n"
     ]
    }
   ],
   "source": [
    "xor_model = MLPModel()\n",
    "L = []\n",
    "lr = 0.01\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    loss = xor_model.forward()\n",
    "    L.append(loss)\n",
    "    xor_model.backward()\n",
    "    xor_model.optimizerSGD(lr)\n",
    "    print(f\"Epoch {epoch} - Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a8e96d",
   "metadata": {},
   "source": [
    "## Display the Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03d6f15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(L)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Iterations')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
