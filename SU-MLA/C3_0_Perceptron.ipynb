{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical work 1 : MLP\n",
    "\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates random data (3 classes)\n",
    "def generate_data_3classes(n1,n2,n3, display=True):\n",
    "    X1 = np.random.randn(n1,2)*0.15 - np.array([0.2 , 0.2])\n",
    "    X2 = np.random.randn(n2,2)*0.25 + np.array([0.1 , 0.5])\n",
    "    X3 = np.random.randn(n3,2)*0.15 + np.array([0.6 , 0.2])\n",
    "    X = np.concatenate((X1,X2,X3), axis=0)\n",
    "    Y = np.concatenate([np.ones(n1), np.ones(n2)*2, np.ones(n3)*3])-1\n",
    "\n",
    "    if display:\n",
    "        plt.plot(X1[:,0], X1[:,1], '.r')\n",
    "        plt.plot(X2[:,0], X2[:,1], '.g')\n",
    "        plt.plot(X3[:,0], X3[:,1], '.b')\n",
    "        plt.show()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# Converts a class vector (integers) to binary class matrix (copy from keras)\n",
    "def to_categorical(y, num_classes=None, dtype=\"float32\"):\n",
    "    y = np.array(y, dtype=\"int\")\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes), dtype=dtype)\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical\n",
    "\n",
    "# plot the classifier boundaries\n",
    "def plot_boundaries( x, W, b, y ):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: np.ndarray\n",
    "        array of training/testing samples\n",
    "    W: np.ndarray\n",
    "        network weight matrix\n",
    "    b: np.ndarray()\n",
    "        network weights\n",
    "    y: np.ndarray\n",
    "        samples labels (array of int)\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    X_train, Y_train = generate_data_3classes(100, 200, 150)\n",
    "    K = 3                      # classes number\n",
    "    D = 2                      # samples dimension\n",
    "    W = 1 * np.random.normal( size=( D, K ) )\n",
    "    b = np.zeros( ( 1, K ) )\n",
    "    plot_boundaries( X_train, W, b, Y_train )\n",
    "    \"\"\"\n",
    "    h = 0.02\n",
    "    x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "    y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid( np.arange( x_min, x_max, h ), np.arange( y_min, y_max, h ) )\n",
    "\n",
    "    Z = np.dot( np.c_[xx.ravel(), yy.ravel()], W ) + b\n",
    "    Z = np.argmax( Z, axis=1 )\n",
    "    Z = Z.reshape( xx.shape )\n",
    "\n",
    "    plt.contourf( xx, yy, Z, cmap=plt.cm.RdYlBu )    \n",
    "    plt.scatter( x[:, 0], x[:, 1], c=y, s=20, cmap=plt.cm.RdYlBu )\n",
    "    plt.title('Decision boundary')\n",
    "    plt.xlim( xx.min(), xx.max() )\n",
    "    plt.ylim( yy.min(), yy.max() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = generate_data_3classes(100, 200, 150)\n",
    "Y_cat = to_categorical(Y_train)\n",
    "\n",
    "K = Y_cat.shape[1] #nombre de classes\n",
    "( num_examples, D ) = X_train.shape # nombre d'exemples et dimension des exemples\n",
    "print( f\"K={K}, num_example={num_examples}, dimension={D}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmo(v):\n",
    "\t...\n",
    "\n",
    "# sigmoid function derivative\n",
    "def sigmop(v):\n",
    "\t..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights random initialization\n",
    "W = 1 * np.random.normal( size=( D, K ) )\n",
    "b = np.zeros( ( 1, K ) )\n",
    "\n",
    "# learning parameters\n",
    "step_size = 1e-1\n",
    "iter_number = 10000\n",
    "\n",
    "plot_boundaries( X_train, W, b, Y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network learning\n",
    "X = X_train\n",
    "Y = Y_train\n",
    "losses = np.zeros( iter_number )\n",
    "\n",
    "for i in range( iter_number ):\n",
    "    # forward (propagation)\n",
    "    v = ...\n",
    "    Y_pred = ...\n",
    "    Err = ...\n",
    "    losses[i] = ...\n",
    "    \n",
    "    if i % 1000 == 0:   \n",
    "        print(f\"iteration {i}: loss {losses[i]}\")\n",
    "\n",
    "    # backward (back-propagation)\n",
    "\n",
    "    ...\n",
    "\n",
    "    dL_W =  ...\n",
    "    dL_b =  ...\n",
    "\n",
    "    W += ...\n",
    "    b += ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss ploting\n",
    "plt.plot(losses)\n",
    "\n",
    "# training accuracy\n",
    "v = ...\n",
    "Y_pred = ...\n",
    "class_pred = ...\n",
    "print( f\"training accuracy: {np.mean(class_pred == Y_train):.2f}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = ...\n",
    "Y_test_cat = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,W,b):\n",
    "    v = ...\n",
    "    return ...\n",
    "\n",
    "def evaluate(Y_pred, Y):\n",
    "    class_pred = ...\n",
    "    print( f\"Accuracy: {np.mean(class_pred == Y):.2}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred = predict(X_train,W,b)\n",
    "Y_test_pred = predict(X_test,W,b)\n",
    "score_train = evaluate(Y_train_pred, Y_train)\n",
    "score_test = evaluate(Y_test_pred, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour aller plus loin\n",
    "* code d'évaluation des performances (et de visualisation)\n",
    "* création d'un jeu de test indépendant (performance en généralisation)\n",
    "* ajouter une couche cachée (obeserver les gain en performance et les frontière de décision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
